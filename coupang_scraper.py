import os
import json
import time
import random
import requests
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from config import Config

class CoupangScraper:
    def __init__(self):
        self.config = Config()
        self.driver = None
        self.results = []
        
    def setup_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            service = webdriver.chrome.service.Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=self.config.get_chrome_options())
            
            # ë´‡ íƒì§€ ìš°íšŒ
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.driver.implicitly_wait(self.config.IMPLICIT_WAIT)
            print("âœ… ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def search_products(self, keyword, max_pages=2):
        """ìƒí’ˆ ê²€ìƒ‰ ë° ë°ì´í„° ìˆ˜ì§‘"""
        try:
            search_url = f"{self.config.SEARCH_URL}?q={keyword}"
            print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: {keyword}")
            
            for page in range(1, max_pages + 1):
                page_url = f"{search_url}&page={page}"
                print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
                
                if self._scrape_page(page_url, page):
                    # í˜ì´ì§€ ê°„ ëœë¤ ë”œë ˆì´
                    time.sleep(random.uniform(2, 5))
                else:
                    print(f"âš ï¸ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨")
                    break
                    
            print(f"âœ… ì´ {len(self.results)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _scrape_page(self, url, page_num):
        """ê°œë³„ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, self.config.PAGE_LOAD_TIMEOUT).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "li[data-component-type='s-search-result']"))
            )
            
            # ìŠ¤í¬ë¡¤ë¡œ ëª¨ë“  ìƒí’ˆ ë¡œë”©
            self._scroll_to_load_products()
            
            # ìƒí’ˆ ìš”ì†Œë“¤ ì°¾ê¸°
            product_elements = self.driver.find_elements(By.CSS_SELECTOR, "li[data-component-type='s-search-result']")
            
            page_results = []
            for idx, element in enumerate(product_elements):
                try:
                    product_data = self._extract_product_data(element, page_num, idx + 1)
                    if product_data:
                        page_results.append(product_data)
                        
                except Exception as e:
                    print(f"âš ï¸ ìƒí’ˆ {idx + 1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            self.results.extend(page_results)
            print(f"ğŸ“¦ í˜ì´ì§€ {page_num}: {len(page_results)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
            return True
            
        except TimeoutException:
            print(f"â° í˜ì´ì§€ {page_num} ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            return False
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def _scroll_to_load_products(self):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ìƒí’ˆ ë¡œë”©"""
        try:
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            while True:
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                # ë¡œë”© ëŒ€ê¸°
                time.sleep(2)
                
                # ìƒˆë¡œìš´ ë†’ì´ í™•ì¸
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
                
        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë¡¤ ë¡œë”© ì˜¤ë¥˜: {str(e)}")
    
    def _extract_product_data(self, element, page_num, item_num):
        """ê°œë³„ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            name_elem = element.find_element(By.CSS_SELECTOR, "div.name")
            name = name_elem.text.strip() if name_elem else "ì •ë³´ ì—†ìŒ"
            
            # ê°€ê²© ì •ë³´
            price_elem = element.find_element(By.CSS_SELECTOR, "strong.price-value")
            price_text = price_elem.text.strip() if price_elem else "0"
            price = self._parse_price(price_text)
            
            # ì›ê°€ (í• ì¸ ì „ ê°€ê²©)
            original_price = price
            try:
                original_elem = element.find_element(By.CSS_SELECTOR, "del.base-price")
                original_price_text = original_elem.text.strip()
                original_price = self._parse_price(original_price_text)
            except NoSuchElementException:
                pass
            
            # í• ì¸ìœ¨
            discount_rate = 0
            if original_price > price:
                discount_rate = round(((original_price - price) / original_price) * 100, 1)
            
            # ìƒí’ˆ ë§í¬
            link_elem = element.find_element(By.CSS_SELECTOR, "a")
            relative_url = link_elem.get_attribute('href') if link_elem else ""
            full_url = relative_url if relative_url.startswith('http') else f"{self.config.BASE_URL}{relative_url}"
            
            # ì´ë¯¸ì§€ URL
            img_elem = element.find_element(By.CSS_SELECTOR, "img")
            image_url = img_elem.get_attribute('src') if img_elem else ""
            
            # í‰ì  ì •ë³´
            rating = 0.0
            review_count = 0
            try:
                rating_elem = element.find_element(By.CSS_SELECTOR, "em.rating")
                rating = float(rating_elem.text.strip()) if rating_elem else 0.0
                
                review_elem = element.find_element(By.CSS_SELECTOR, "span.rating-total-review")
                review_text = review_elem.text.strip().replace('(', '').replace(')', '').replace(',', '')
                review_count = int(review_text) if review_text.isdigit() else 0
            except (NoSuchElementException, ValueError):
                pass
            
            # ë°°ì†¡ ì •ë³´
            shipping_info = "ì¼ë°˜ë°°ì†¡"
            try:
                rocket_elem = element.find_element(By.CSS_SELECTOR, ".rocket-badge")
                if rocket_elem:
                    shipping_info = "ë¡œì¼“ë°°ì†¡"
            except NoSuchElementException:
                pass
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            product_data = {
                'product_name': name,
                'price': price,
                'original_price': original_price,
                'discount_rate': f"{discount_rate}%",
                'url': full_url,
                'image_url': image_url,
                'rating': rating,
                'review_count': review_count,
                'shipping': shipping_info,
                'page': page_num,
                'position': item_num,
                'scraped_at': datetime.now().isoformat(),
                'source': 'coupang'
            }
            
            return product_data
            
        except Exception as e:
            print(f"âŒ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _parse_price(self, price_text):
        """ê°€ê²© í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜"""
        try:
            # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì ì œê±°
            cleaned_price = ''.join(filter(str.isdigit, price_text))
            return int(cleaned_price) if cleaned_price else 0
        except:
            return 0
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        try:
            # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs('results', exist_ok=True)
            
            if not self.results:
                print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(self.results)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            keyword_safe = self.config.KEYWORD.replace(' ', '_')
            
            # JSON ì €ì¥
            json_filename = f"results/coupang_{keyword_safe}_{timestamp}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            
            # CSV ì €ì¥
            csv_filename = f"results/coupang_{keyword_safe}_{timestamp}.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            # ìš”ì•½ í†µê³„
            summary = {
                'keyword': self.config.KEYWORD,
                'total_products': len(self.results),
                'avg_price': int(df['price'].mean()) if len(df) > 0 else 0,
                'min_price': int(df['price'].min()) if len(df) > 0 else 0,
                'max_price': int(df['price'].max()) if len(df) > 0 else 0,
                'scraped_at': datetime.now().isoformat(),
                'files': {
                    'json': json_filename,
                    'csv': csv_filename
                }
            }
            
            # ìš”ì•½ ì €ì¥
            summary_filename = f"results/summary_{keyword_safe}_{timestamp}.json"
            with open(summary_filename, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            print(f"   - JSON: {json_filename}")
            print(f"   - CSV: {csv_filename}")
            print(f"   - ìš”ì•½: {summary_filename}")
            
            return summary
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def send_to_webhook(self, summary):
        """n8n webhookìœ¼ë¡œ ê²°ê³¼ ì „ì†¡"""
        if not self.config.WEBHOOK_URL:
            print("â„¹ï¸ Webhook URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return True
            
        try:
            payload = {
                'status': 'success',
                'summary': summary,
                'products': self.results[:10],  # ìƒìœ„ 10ê°œë§Œ ì „ì†¡
                'message': f"ì¿ íŒ¡ '{self.config.KEYWORD}' ê²€ìƒ‰ ì™„ë£Œ: {len(self.results)}ê°œ ìƒí’ˆ"
            }
            
            response = requests.post(
                self.config.WEBHOOK_URL,
                json=payload,
                headers={'Content-Type': 'application/json'},
                timeout=30
            )
            
            if response.status_code == 200:
                print("ğŸ“¡ Webhook ì „ì†¡ ì„±ê³µ")
                return True
            else:
                print(f"âš ï¸ Webhook ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Webhook ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            print("ğŸ§¹ ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = CoupangScraper()
    
    try:
        print("ğŸš€ ì¿ íŒ¡ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“ ê²€ìƒ‰ì–´: {scraper.config.KEYWORD}")
        print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {scraper.config.MAX_PAGES}")
        
        # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        if not scraper.setup_driver():
            return
        
        # ìƒí’ˆ ê²€ìƒ‰
        if scraper.search_products(scraper.config.KEYWORD, scraper.config.MAX_PAGES):
            # ê²°ê³¼ ì €ì¥
            summary = scraper.save_results()
            
            if summary:
                # Webhook ì „ì†¡
                scraper.send_to_webhook(summary)
                print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            else:
                print("âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
        else:
            print("âŒ ìƒí’ˆ ê²€ìƒ‰ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    main()